"""
Universal LLM client -- uses NeuroAPI (OpenAI-compatible) for text generation.
GigaChat Embeddings are kept separately in services/rag_service.py.

Backward-compatible: class name and public interface unchanged.
"""

import json
import logging
import asyncio
from openai import OpenAI

from config import (
    LLM_API_KEY,
    LLM_BASE_URL,
    LLM_MODELS,
    LLM_TOKEN_LIMITS,
    SYSTEM_PROMPT_PATH,
)

logger = logging.getLogger(__name__)


def _load_system_prompt():
    try:
        with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ system prompt: {e}")
        return "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ JSON."


class GigaChatClient:
    """LLM client using NeuroAPI (OpenAI-compatible).
    Class name kept for backward compatibility with all imports."""

    def __init__(self, model_type: str = "standard"):
        self.model_type = model_type
        self.model_name = LLM_MODELS.get(model_type, LLM_MODELS.get("standard", "gemini-2.5-pro"))
        self.client: OpenAI | None = None
        self.system_prompt = _load_system_prompt()
        self._initialize_client()

    def _initialize_client(self):
        """Initialize the OpenAI-compatible client for NeuroAPI."""
        try:
            logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM ({self.model_name}) —á–µ—Ä–µ–∑ NeuroAPI")

            self.client = OpenAI(
                api_key=LLM_API_KEY,
                base_url=LLM_BASE_URL,
                timeout=60,
            )

            # Quick connectivity test
            test = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: OK"}],
                max_tokens=5,
            )
            reply = test.choices[0].message.content.strip() if test.choices else "?"
            logger.info(f"‚úÖ LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name}")
            logger.info(f"üìä –¢–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç: {reply}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM ({self.model_name}): {e}")
            self.client = None

    def set_model(self, model_type: str):
        """Switch the active model tier."""
        self.model_type = model_type
        self.model_name = LLM_MODELS.get(model_type, LLM_MODELS.get("standard", "gemini-2.5-pro"))
        logger.info(f"üîÑ –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –Ω–∞: {self.model_name} (tier={model_type})")
        # No need to re-create client -- just change model name, same API key.

    # ------------------------------------------------------------------
    # generate_recommendation
    # ------------------------------------------------------------------

    def generate_recommendation(self, startup: dict, user_request: str = "", query_history=None) -> str:
        """Generate AI recommendation for a startup (standard and premium tiers)."""
        if not self.client:
            return ""

        limits = LLM_TOKEN_LIMITS.get(self.model_type, LLM_TOKEN_LIMITS.get("standard", {}))
        max_tokens = limits.get("recommendations", 0)
        if max_tokens <= 0:
            return ""

        try:
            few_shot_text = ""
            try:
                from services.few_shot_examples import get_few_shot_prompt
                history_patterns = []
                if query_history:
                    history_patterns = query_history.get_query_patterns(user_request)
                few_shot_text = get_few_shot_prompt(user_request, history_patterns)
                if few_shot_text:
                    logger.info("‚úÖ Few-shot –ø—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –ø—Ä–æ–º–ø—Ç")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Few-shot –ø—Ä–∏–º–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")

            startup_info = f"""
–û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
–ù–∞–∑–≤–∞–Ω–∏–µ: {startup.get('name', '–Ω/–¥')}
–ö–ª–∞—Å—Ç–µ—Ä: {startup.get('cluster', '–Ω/–¥')}
–ì–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω–∏—è: {startup.get('year', '–Ω/–¥')}
–°—Ç–∞—Ç—É—Å: {startup.get('status', '–Ω/–¥')}

–û–ü–ò–°–ê–ù–ò–ï:
{(startup.get('company_description', '') or startup.get('description', ''))[:400]}

–ü–†–û–î–£–ö–¢–´ –ò –ü–†–û–ï–ö–¢–´:
–ü—Ä–æ–¥—É–∫—Ç—ã: {str(startup.get('product_names', '–Ω/–¥'))[:200]}
–ü—Ä–æ–µ–∫—Ç—ã: {str(startup.get('project_names', '–Ω/–¥'))[:200]}
–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {str(startup.get('technologies', '–Ω/–¥'))[:200]}
–û—Ç—Ä–∞—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: {str(startup.get('industries', '–Ω/–¥'))[:200]}

–¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ó–†–ï–õ–û–°–¢–¨:
TRL: {startup.get('trl', '–Ω/–¥')}
IRL: {startup.get('irl', '–Ω/–¥')} - {str(startup.get('irl_description', ''))[:150]}
MRL: {startup.get('mrl', '–Ω/–¥')}
CRL: {startup.get('crl', '–Ω/–¥')} - {str(startup.get('crl_description', ''))[:150]}

–§–ò–ù–ê–ù–°–´:
–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å: {startup.get('analysis', {}).get('AvgProfit', 0) / 1_000_000:.2f} –º–ª–Ω —Ä—É–±
–î–∏–Ω–∞–º–∏–∫–∞: {startup.get('analysis', {}).get('FinancialStability', '–Ω/–¥')}
–§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ: {startup.get('analysis', {}).get('FinancialHealth', '–Ω/–¥')}

–ü–ê–¢–ï–ù–¢–´ –ò –ò–°:
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç–µ–Ω—Ç–æ–≤: {startup.get('patent_count', 0)}
–î–µ—Ç–∞–ª–∏: {str(startup.get('patents', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'))[:300]}
"""
            prompt = f"""–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ä—Ç–∞–ø–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–ø—Ä–æ—Å–∞.

{startup_info}

–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{user_request}
{few_shot_text}

–ó–ê–î–ê–ß–ê:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–æ–º–ø–∞–Ω–∏—è –∏ –µ—ë —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∑–∞–ø—Ä–æ—Å—É. –î–∞–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤. –û–ø–∏—Ä–∞–π—Å—è —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞.

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê (–±–µ–∑ markdown, –±–µ–∑ **, –±–µ–∑ _):

–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
‚Ä¢ [—Ñ–∞–∫—Ç —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏]
‚Ä¢ [–µ—â—ë —Ñ–∞–∫—Ç ‚Äî –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è]

–†–∏—Å–∫–∏:
‚Ä¢ [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ä–∏—Å–∫ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º]

–≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:
[3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –û–±—ä—è—Å–Ω–∏, —á–µ–º –∫–æ–º–ø–∞–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–ø—Ä–æ—Å–∞. –£–∫–∞–∂–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è. –û—Ü–µ–Ω–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–∞–∑–≤–∏—Ç–∏—è –≤ –Ω—É–∂–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏. –ï—Å–ª–∏ –ø—Ä—è–º–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç ‚Äî –æ–±—ä—è—Å–Ω–∏, –≤ –∫–∞–∫–æ–π —Å–º–µ–∂–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏—è —Å–∏–ª—å–Ω–∞.]

–ü–†–ê–í–ò–õ–ê:
- –§–æ–∫—É—Å –Ω–∞ –í–û–ó–ú–û–ñ–ù–û–°–¢–Ø–•, –∞ –Ω–µ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö
- –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è ‚Üí "–∫–æ–º–ø–∞–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–º–µ–∂–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏", –∞ –Ω–µ "–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç"
- –¶–∏—Ñ—Ä—ã: –ø—Ä–∏–±—ã–ª—å, –≤—ã—Ä—É—á–∫–∞, TRL/IRL/MRL/CRL, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç–µ–Ω—Ç–æ–≤
- –ù–ï –¥–∞–≤–∞–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown-—Ä–∞–∑–º–µ—Ç–∫—É
- –ù–ï –æ–±—Ä–µ–∑–∞–π –º—ã—Å–ª—å –Ω–∞ –ø–æ–ª—É—Å–ª–æ–≤–µ ‚Äî –∑–∞–≤–µ—Ä—à–∏ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"""

            temperature = limits.get("temperature_recommendations", 0.5)

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
            )

            if response.choices:
                recommendation = response.choices[0].message.content.strip()
                recommendation = recommendation.replace("**", "").replace("__", "").replace("*", "").replace("_", "")
                logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è ({len(recommendation)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return recommendation

            return ""

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
            return ""

    # ------------------------------------------------------------------
    # get_startup_filters
    # ------------------------------------------------------------------

    def get_startup_filters(self, user_request: str, user_repository=None, user_id=None):
        """Convert user query into structured filters via LLM."""
        logger.info(f"üì® –ó–∞–ø—Ä–æ—Å –∫ LLM ({self.model_name}): {user_request}")

        limits = LLM_TOKEN_LIMITS.get(self.model_type, {})
        max_tokens = limits.get("filters", 0)

        # Some tiers skip LLM and use fallback (cheaper)
        if max_tokens <= 0 or not self.client:
            logger.info(f"üîÑ Tier {self.model_type}: –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback-—Ñ–∏–ª—å—Ç—Ä—ã (RAG –Ω–∞–π–¥–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ)")
            fallback = self._get_fallback_filters(user_request)
            self._soften_filters(fallback)
            return fallback

        try:
            temperature = limits.get("temperature_filters", 0.2)

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_request},
                ],
                max_tokens=max_tokens,
                temperature=temperature,
            )

            if not response.choices:
                logger.error("‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
                return self._get_fallback_filters(user_request)

            json_string = response.choices[0].message.content
            logger.info(f"üì• –û—Ç–≤–µ—Ç LLM: {json_string}")

            json_string = self._clean_json_response(json_string)
            filters = json.loads(json_string)

            # Soften filters for economy/standard tiers
            if self.model_type in ("economy", "standard"):
                self._soften_filters(filters)

            filters = self._clean_empty_filters(filters, user_request)

            if not self._validate_filters(filters):
                logger.error("‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤")
                return self._get_fallback_filters(user_request)

            # Token tracking
            tokens_used = 0
            if hasattr(response, "usage") and response.usage:
                tokens_used = response.usage.total_tokens
            logger.info(f"‚úÖ –§–∏–ª—å—Ç—Ä—ã –ø–æ–ª—É—á–µ–Ω—ã ({self.model_name}), —Ç–æ–∫–µ–Ω–æ–≤: {tokens_used}")

            if user_repository and user_id and tokens_used > 0:
                try:
                    asyncio.create_task(
                        user_repository.add_token_usage(user_id, self.model_type, tokens_used, user_request[:200])
                    )
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")

            return filters

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ JSON: {e}")
            return self._get_fallback_filters(user_request)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
            return self._get_fallback_filters(user_request)

    # ------------------------------------------------------------------
    # Helpers (unchanged logic, just cleaned up)
    # ------------------------------------------------------------------

    @staticmethod
    def _soften_filters(filters: dict):
        """Remove strict criteria so RAG can find relevant startups."""
        for key in ("DeepTech", "GenAI", "WOW"):
            filters[key] = ""
        for key in ("trl", "irl", "mrl", "crl", "stage", "cluster", "category"):
            filters[key] = []
        filters["min_profit"] = 0

    @staticmethod
    def _clean_json_response(json_string: str) -> str:
        json_string = json_string.replace("```json", "").replace("```", "").strip()
        json_string = " ".join(json_string.split())
        return json_string

    def _clean_empty_filters(self, filters: dict, user_request: str) -> dict:
        fallback = self._get_fallback_filters(user_request)
        if not filters.get("DeepTech"):
            filters["DeepTech"] = fallback["DeepTech"]
        if not filters.get("GenAI"):
            filters["GenAI"] = fallback["GenAI"]
        if not filters.get("WOW"):
            filters["WOW"] = fallback["WOW"]
        for key in ("trl", "irl", "mrl", "crl", "year", "country", "category", "stage", "cluster", "status"):
            if not filters.get(key):
                filters[key] = fallback.get(key, [])
        if "min_profit" not in filters or filters.get("min_profit") is None:
            filters["min_profit"] = fallback.get("min_profit", 0)
        if "has_patents" not in filters:
            filters["has_patents"] = fallback.get("has_patents", False)
        return filters

    @staticmethod
    def _validate_filters(filters: dict) -> bool:
        required = {"DeepTech", "GenAI", "WOW", "trl", "irl", "mrl", "crl", "year", "country", "category", "stage", "min_profit"}
        if not isinstance(filters, dict):
            return False
        missing = required - filters.keys()
        if missing:
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏: {missing}")
            return False
        dt = filters.get("DeepTech")
        if dt != "" and not isinstance(dt, (int, str)):
            return False
        if isinstance(dt, str) and dt != "" and not dt.isdigit():
            return False
        if filters.get("GenAI") not in ("–µ—Å—Ç—å", "–Ω–µ—Ç", ""):
            return False
        if filters.get("WOW") not in ("–¥–∞", "–Ω–µ—Ç", ""):
            return False
        if not isinstance(filters.get("min_profit"), (int, float)):
            return False
        return True

    def _get_fallback_filters(self, user_request: str = ""):
        """Smart keyword-based fallback filters."""
        logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ fallback-—Ñ–∏–ª—å—Ç—Ä—ã")
        q = user_request.lower()

        is_bad = any(w in q for w in ("–ø–ª–æ—Ö–æ–π", "—Å–ª–∞–±—ã–π", "–Ω–∏–∑–∫–∏–π", "–ø–ª–æ—Ö"))
        is_good = any(w in q for w in ("—Ö–æ—Ä–æ—à–∏–π", "—Å–∏–ª—å–Ω—ã–π", "–≤—ã—Å–æ–∫–∏–π", "–ª—É—á—à", "–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤"))

        min_profit = 0
        for phrase, val in [
            ("–±–æ–ª–µ–µ 100 –º–ª–Ω", 100_000_000), ("–±–æ–ª—å—à–µ 100 –º–ª–Ω", 100_000_000),
            ("–±–æ–ª–µ–µ 50 –º–ª–Ω", 50_000_000), ("–±–æ–ª—å—à–µ 50 –º–ª–Ω", 50_000_000),
            ("–±–æ–ª–µ–µ 10 –º–ª–Ω", 10_000_000), ("–±–æ–ª—å—à–µ 10 –º–ª–Ω", 10_000_000),
            ("–±–æ–ª–µ–µ 5 –º–ª–Ω", 5_000_000), ("–±–æ–ª–µ–µ 1 –º–ª–Ω", 1_000_000), ("–ø—Ä–∏–±—ã–ª—å–Ω", 1_000_000),
        ]:
            if phrase in q:
                min_profit = val
                break

        cluster, category, country = [], [], []
        for kw_list, val in [
            (("–∏—Ç", "it", "—Å–æ—Ñ—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º", "digital", "—Ü–∏—Ñ—Ä–æ–≤"), ["–ò–¢"]),
            (("–±–∏–æ–º–µ–¥", "–º–µ–¥–∏—Ü–∏–Ω", "–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω", "—Ñ–∞—Ä–º"), ["–ë–∏–æ–º–µ–¥–∏—Ü–∏–Ω–∞"]),
            (("—ç–Ω–µ—Ä–≥", "—ç–Ω–µ—Ä–≥–æ—Ç–µ—Ö"), ["–≠–Ω–µ—Ä–≥–æ—Ç–µ—Ö"]),
        ]:
            if any(w in q for w in kw_list):
                cluster = val
                break
        for kw_list, val in [
            (("–∏—Ç", "it", "—Å–æ—Ñ—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º"), ["–ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ï –¢–ï–•–ù–û–õ–û–ì–ò–ò"]),
            (("–º–µ–¥–∏—Ü–∏–Ω", "–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω", "—Ñ–∞—Ä–º", "–±–∏–æ–º–µ–¥"), ["–ó–î–†–ê–í–û–û–•–†–ê–ù–ï–ù–ò–ï"]),
            (("—Ñ–∏–Ω–∞–Ω—Å", "—Ñ–∏–Ω—Ç–µ—Ö", "–±–∞–Ω–∫"), ["–§–ò–ù–ê–ù–°–û–í–´–ô –°–ï–ö–¢–û–†"]),
            (("–ø—Ä–æ–º—ã—à–ª", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤"), ["–ü–†–û–ú–´–®–õ–ï–ù–ù–û–°–¢–¨"]),
        ]:
            if any(w in q for w in kw_list):
                category = val
                break
        for kw, val in [("—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"), ("—Å–ø–±", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"),
                        ("–º–æ—Å–∫–≤", "–ú–æ—Å–∫–≤–∞"), ("–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥")]:
            if kw in q:
                country = [val]
                break

        has_patents = None
        if any(w in q for w in ("–ø–∞—Ç–µ–Ω—Ç", "–∑–∞—â–∏—â–µ–Ω", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω")):
            has_patents = True

        keyword_search = ""
        if not category:
            exclude = {"–ø—Ä–æ–µ–∫—Ç", "—Å–≤—è–∑–∞–Ω–Ω—ã–π", "–≥–æ–¥–æ–≤–æ–π", "–ø—Ä–∏–±—ã–ª—å—é", "–±–æ–ª–µ–µ", "–º–ª–Ω", "—Ä—É–±", "—Å—Ç–∞—Ä—Ç–∞–ø", "–∫–æ–º–ø–∞–Ω–∏—è"}
            words = [w for w in q.split() if len(w) > 4 and w not in exclude]
            keyword_search = " ".join(words[:3])

        if is_bad:
            return {"DeepTech": 1, "GenAI": "–Ω–µ—Ç", "WOW": "–Ω–µ—Ç",
                    "trl": ["1-3"], "irl": ["1-3"], "mrl": ["1-3"], "crl": ["1-3"],
                    "year": ["2015-2025"], "country": country, "category": category,
                    "cluster": cluster, "stage": [], "status": ["active"],
                    "min_profit": min_profit, "has_patents": has_patents, "keyword_search": keyword_search}
        if is_good:
            return {"DeepTech": 3, "GenAI": "–µ—Å—Ç—å", "WOW": "–¥–∞",
                    "trl": ["7-9"], "irl": ["7-9"], "mrl": ["7-9"], "crl": ["7-9"],
                    "year": ["2018-2025"], "country": country, "category": category,
                    "cluster": cluster, "stage": [], "status": ["active"],
                    "min_profit": min_profit, "has_patents": has_patents, "keyword_search": keyword_search}

        return {"DeepTech": "", "GenAI": "", "WOW": "",
                "trl": [], "irl": [], "mrl": [], "crl": [],
                "year": [], "country": country, "category": category,
                "cluster": cluster, "stage": [], "status": ["active"],
                "min_profit": min_profit, "has_patents": has_patents, "keyword_search": keyword_search}
