"""
RAG-—Å–µ—Ä–≤–∏—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GigaChat Embeddings –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""
import json
import numpy as np
from typing import List, Dict, Tuple
from gigachat import GigaChat
from config import GIGACHAT_API_TOKEN
from logger import logger

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("‚ö†Ô∏è scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, TF-IDF fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")


class RAGService:
    def __init__(self):
        self.giga = None
        self.embeddings_cache = {}  # –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.startup_vectors = []  # –°–ø–∏—Å–æ–∫ (startup_id, vector)
        self.startup_texts = {}  # –°–ª–æ–≤–∞—Ä—å {startup_id: full_text}
        self.use_tfidf = False  # –§–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è TF-IDF
        self.tfidf_vectorizer = None
        self._initialize_client()
    
    def _initialize_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChat –¥–ª—è embeddings"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GigaChat –¥–ª—è embeddings
            logger.info("üí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GigaChat Embeddings (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)")
            self.giga = GigaChat(
                credentials=GIGACHAT_API_TOKEN,
                scope="GIGACHAT_API_PERS",
                verify_ssl_certs=False,
                timeout=60  # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–ª—è embeddings
            )
            self.use_tfidf = False
            logger.info("‚úÖ RAG Service: GigaChat Embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # TF-IDF –∫–∞–∫ fallback
            if SKLEARN_AVAILABLE:
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=1000,
                    ngram_range=(1, 2),
                    min_df=2,
                    max_df=0.8,
                    sublinear_tf=True
                )
                logger.info("‚úÖ TF-IDF fallback –≥–æ—Ç–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GigaChat Embeddings: {e}")
            logger.info("üí° –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ TF-IDF fallback")
            self.giga = None
            self.use_tfidf = True
            
            if SKLEARN_AVAILABLE:
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=1000,
                    ngram_range=(1, 2),
                    min_df=2,
                    max_df=0.8,
                    sublinear_tf=True
                )
                logger.info("‚úÖ RAG Service: TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞")
            else:
                logger.error("‚ùå –ù–∏ GigaChat Embeddings, –Ω–∏ TF-IDF –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    def _expand_query_with_gigachat(self, query: str) -> str:
        """
        –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ GigaChat
        –í–º–µ—Å—Ç–æ –∂–µ—Å—Ç–∫–æ –ø—Ä–æ–ø–∏—Å–∞–Ω–Ω—ã—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤, GigaChat —Å–∞–º –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        –ü—Ä–∏–º–µ—Ä:
        - –ó–∞–ø—Ä–æ—Å: "–¥—Ä–æ–Ω—ã" ‚Üí "–¥—Ä–æ–Ω –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫ –±–ø–ª–∞ –∫–≤–∞–¥—Ä–æ–∫–æ–ø—Ç–µ—Ä uav"
        - –ó–∞–ø—Ä–æ—Å: "–∞–≤—Ç–æ–º–æ–±–∏–ª–∏" ‚Üí "–∞–≤—Ç–æ–º–æ–±–∏–ª—å –º–∞—à–∏–Ω–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∞–≤—Ç–æ"
        - –ó–∞–ø—Ä–æ—Å: "AI" ‚Üí "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
        """
        if not self.giga:
            return ""
        
        try:
            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏.
–î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–æ–±–∞–≤—å —Å–∏–Ω–æ–Ω–∏–º—ã, –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è.

–ó–ê–ü–†–û–°: {query}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–¥—Ä–æ–Ω—ã", "AI", "–º–µ–¥–∏—Ü–∏–Ω–∞")
2. –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–æ–±–∞–≤—å:
   - –°–∏–Ω–æ–Ω–∏–º—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
   - –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
   - –ü–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π):
[–∫–ª—é—á–µ–≤–æ–µ_—Å–ª–æ–≤–æ1] [—Å–∏–Ω–æ–Ω–∏–º1] [—Å–∏–Ω–æ–Ω–∏–º2] [–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞] [—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π_—Ç–µ—Ä–º–∏–Ω] ...

–ü–†–ò–ú–ï–†–´:
–ó–∞–ø—Ä–æ—Å: "—Å—Ç–∞—Ä—Ç–∞–ø —Å –¥—Ä–æ–Ω–∞–º–∏"
–û—Ç–≤–µ—Ç: –¥—Ä–æ–Ω –¥—Ä–æ–Ω—ã –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã–π –±–ø–ª–∞ –±–ª–∞ uav –∫–≤–∞–¥—Ä–æ–∫–æ–ø—Ç–µ—Ä –∫–≤–∞–¥—Ä–∞–∫–æ–ø—Ç–µ—Ä –º—É–ª—å—Ç–∏–∫–æ–ø—Ç–µ—Ä –æ–∫—Ç–æ–∫–æ–ø—Ç–µ—Ä –∫–æ–ø—Ç–µ—Ä –ª–µ—Ç–∞—Ç–µ–ª—å–Ω—ã–π

–ó–∞–ø—Ä–æ—Å: "AI –≤ –º–µ–¥–∏—Ü–∏–Ω–µ"
–û—Ç–≤–µ—Ç: –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç ai –Ω–µ–π—Ä–æ—Å–µ—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ml deep learning –º–µ–¥–∏—Ü–∏–Ω–∞ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ª–µ—á–µ–Ω–∏–µ"""

            from gigachat.models import Chat, Messages, MessagesRole
            
            response = self.giga.chat(Chat(
                messages=[Messages(role=MessagesRole.USER, content=prompt)],
                temperature=0.3,
                max_tokens=150
            ))
            
            if response and response.choices:
                expanded = response.choices[0].message.content.strip().lower()
                logger.info(f"ü§ñ GigaChat —Ä–∞—Å—à–∏—Ä–∏–ª –∑–∞–ø—Ä–æ—Å: '{query}' ‚Üí '{expanded[:100]}...'")
                return expanded
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ GigaChat: {e}")
            return ""
    
    def _create_startup_text(self, startup: dict) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—Ç–∞–ø–∞
        –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        parts = []
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –∫–ª–∞—Å—Ç–µ—Ä (–≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞)
        if startup.get("name"):
            parts.append(f"–ö–æ–º–ø–∞–Ω–∏—è: {startup['name']}")
        if startup.get("cluster"):
            parts.append(f"–ö–ª–∞—Å—Ç–µ—Ä: {startup['cluster']}")
        
        # –û–ø–∏—Å–∞–Ω–∏—è (–æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç)
        if startup.get("company_description"):
            parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {startup['company_description'][:500]}")
        elif startup.get("description"):
            parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {startup['description'][:500]}")
        
        if startup.get("product_description"):
            parts.append(f"–ü—Ä–æ–¥—É–∫—Ç—ã: {startup['product_description'][:400]}")
        
        if startup.get("project_description"):
            parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ–≤: {startup['project_description'][:400]}")
        
        # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞)
        if startup.get("technologies"):
            parts.append(f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {startup['technologies'][:300]}")
        
        if startup.get("product_names"):
            parts.append(f"–ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {startup['product_names'][:200]}")
        
        if startup.get("project_names"):
            parts.append(f"–ü—Ä–æ–µ–∫—Ç—ã: {startup['project_names'][:200]}")
        
        # –û—Ç—Ä–∞—Å–ª–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if startup.get("industries"):
            parts.append(f"–û—Ç—Ä–∞—Å–ª–∏: {startup['industries'][:200]}")
        
        if startup.get("category"):
            parts.append(f"–°—Ñ–µ—Ä—ã: {startup['category']}")
        
        # IRL/CRL –æ–ø–∏—Å–∞–Ω–∏—è (—Å–æ–¥–µ—Ä–∂–∞—Ç —Ü–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é)
        if startup.get("irl_description"):
            parts.append(f"IRL: {startup['irl_description'][:150]}")
        
        if startup.get("crl_description"):
            parts.append(f"CRL: {startup['crl_description'][:150]}")
        
        return " ".join(parts)
    
    def get_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ GigaChat"""
        if not self.giga:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]
        
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            text = text[:2000]
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ GigaChat Embeddings API
            response = self.giga.embeddings([text])
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            if response and hasattr(response, 'data') and len(response.data) > 0:
                embedding_data = response.data[0]
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä
                if hasattr(embedding_data, 'embedding'):
                    vector = np.array(embedding_data.embedding)
                elif isinstance(embedding_data, dict) and 'embedding' in embedding_data:
                    vector = np.array(embedding_data['embedding'])
                else:
                    vector = np.array(embedding_data)
                
                self.embeddings_cache[text] = vector
                return vector
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ—Ç GigaChat: {e}")
            logger.info("üí° –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TF-IDF fallback")
            return None
    
    def index_startups(self, startups: List[dict], progress_callback=None):
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ - —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤
        """
        logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {len(startups)} —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤...")
        
        self.startup_vectors = []
        self.startup_texts = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        texts = []
        startup_ids = []
        
        for startup in startups:
            startup_id = startup.get("id", "")
            if not startup_id:
                continue
            
            full_text = self._create_startup_text(startup)
            self.startup_texts[startup_id] = full_text
            texts.append(full_text)
            startup_ids.append(startup_id)
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if self.use_tfidf and self.tfidf_vectorizer:
            # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–±—ã—Å—Ç—Ä–æ, –±–µ–∑ API)
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...")
            try:
                vectors = self.tfidf_vectorizer.fit_transform(texts).toarray()
                self.startup_vectors = [(startup_ids[i], vectors[i]) for i in range(len(startup_ids))]
                logger.info(f"‚úÖ TF-IDF: –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(self.startup_vectors)} —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        else:
            # GigaChat Embeddings (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ)
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GigaChat Embeddings...")
            for i, (startup_id, text) in enumerate(zip(startup_ids, texts)):
                try:
                    vector = self.get_embedding(text)
                    if vector is not None:
                        self.startup_vectors.append((startup_id, vector))
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if progress_callback and (i + 1) % 100 == 0:
                        progress_callback(i + 1, len(texts))
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞: {e}")
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(self.startup_vectors)} —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤")
        return len(self.startup_vectors)
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return dot_product / (norm1 * norm2)
        except:
            return 0.0
    
    def semantic_search(self, query: str, top_k: int = 50) -> List[Tuple[str, float]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (startup_id, similarity_score)
        """
        if len(self.startup_vectors) == 0:
            logger.warning("RAG Service –Ω–µ –≥–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
            if self.use_tfidf and self.tfidf_vectorizer:
                # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
                query_vector = self.tfidf_vectorizer.transform([query]).toarray()[0]
            else:
                # GigaChat Embeddings
                query_vector = self.get_embedding(query)
                if query_vector is None:
                    return []
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞–º–∏
            similarities = []
            for startup_id, startup_vector in self.startup_vectors:
                similarity = self.cosine_similarity(query_vector, startup_vector)
                similarities.append((startup_id, similarity))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
            top_results = similarities[:top_k]
            logger.info(f"üîç RAG –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(top_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ (–º–∞–∫—Å. —Å—Ö–æ–¥—Å—Ç–≤–æ: {top_results[0][1]:.3f})")
            
            return top_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def hybrid_search(self, query: str, filters: dict, all_startups: List[dict], 
                     top_k: int = 50, filter_functions: dict = None) -> List[dict]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + –ú–Ø–ì–ö–ò–ï —Ñ–∏–ª—å—Ç—Ä—ã
        
        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê:
        1. –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ GigaChat (–ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ ‚Üí —Ç–æ–ø-100 –ø–æ similarity
        3. –î–æ–±–∞–≤–ª—è–µ–º similarity score –∫–æ –≤—Å–µ–º
        4. –ü—Ä–∏–º–µ–Ω—è–µ–º –¢–û–õ–¨–ö–û –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (status, max_profit_limit, —Å—Ç–∞—Ä—Ç–∞–ø)
        5. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity
        6. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω—ã
        
        –¶–µ–ª—å: –í–°–ï–ì–î–ê –≤—ã–≤–æ–¥–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ relevance
        """
        # 1. –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        expanded_query = self._expand_query_with_gigachat(query)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        search_query = f"{query} {expanded_query}" if expanded_query else query
        logger.info(f"üîç –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π): {search_query[:150]}...")
        
        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–±–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –∑–∞–ø–∞—Å–∞ –ø–µ—Ä–µ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π)
        # top_k —É–∂–µ —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 200, —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1.5 –¥–ª—è –∑–∞–ø–∞—Å–∞
        semantic_results = self.semantic_search(search_query, top_k=int(top_k * 1.5))
        
        if not semantic_results:
            logger.warning("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return []
        
        # 2. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ä—Ç–∞–ø—ã –ø–æ ID –∏ –¥–æ–±–∞–≤–ª—è–µ–º similarity
        startup_ids = {startup_id for startup_id, _ in semantic_results}
        candidate_startups = [s for s in all_startups if s.get("id") in startup_ids]
        
        # –î–æ–±–∞–≤–ª—è–µ–º similarity score –∫ –∫–∞–∂–¥–æ–º—É —Å—Ç–∞—Ä—Ç–∞–ø—É
        similarity_map = {startup_id: score for startup_id, score in semantic_results}
        for startup in candidate_startups:
            startup['rag_similarity'] = similarity_map.get(startup.get("id", ""), 0)
        
        logger.info(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {len(candidate_startups)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –¢–û–õ–¨–ö–û –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (–Ω–µ –æ—Ç—Å–µ–∫–∞–µ–º –≤—Å–µ!)
        filtered_startups = candidate_startups
        
        if filter_functions:
            max_profit_limit = filters.get("max_profit_limit")
            get_max_profit = filter_functions.get('get_max_profit')
            determine_stage = filter_functions.get('determine_stage')
            
            # –ö—Ä–∏—Ç–∏—á–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä 1: max_profit_limit (—É–±–∏—Ä–∞–µ–º –∑—Ä–µ–ª—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏)
            if max_profit_limit and get_max_profit:
                count_before = len(filtered_startups)
                filtered_startups = [s for s in filtered_startups if get_max_profit(s) <= max_profit_limit]
                logger.info(f"üîç –§–∏–ª—å—Ç—Ä 'max_profit_limit' (<= {max_profit_limit/1_000_000:.0f}M): {count_before} -> {len(filtered_startups)}")
            
            # –ö—Ä–∏—Ç–∏—á–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä 2: status (—Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ)
            status_filter = filters.get("status", [])
            if status_filter and len(status_filter) > 0:
                count_before = len(filtered_startups)
                filtered_startups = [
                    s for s in filtered_startups
                    if s.get("status", "").lower() in [st.lower() for st in status_filter]
                ]
                logger.info(f"üîç –§–∏–ª—å—Ç—Ä 'status': {count_before} -> {len(filtered_startups)}")
            
            # –ö—Ä–∏—Ç–∏—á–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä 3: –∏—Å–∫–ª—é—á–∞–µ–º –∑—Ä–µ–ª—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ "—Å—Ç–∞—Ä—Ç–∞–ø"
            query_lower = query.lower()
            if "—Å—Ç–∞—Ä—Ç–∞–ø" in query_lower and determine_stage:
                count_before = len(filtered_startups)
                filtered_startups = [
                    s for s in filtered_startups 
                    if determine_stage(s) in ["Pre-seed", "Seed", "Round A"]
                ]
                logger.info(f"üéØ –§–∏–ª—å—Ç—Ä '—Å—Ç–∞—Ä—Ç–∞–ø': {count_before} -> {len(filtered_startups)}")
        
        # 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity (–ì–õ–ê–í–ù–û–ï - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å!)
        filtered_startups.sort(
            key=lambda s: s.get('rag_similarity', 0),
            reverse=True
        )
        
        logger.info(f"‚úÖ –ü–æ—Å–ª–µ –ú–Ø–ì–ö–û–ô —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_startups)} —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤")
        
        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K –ø–æ similarity, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        # –¶–µ–ª—å: –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
        return filtered_startups[:top_k]
    
    def save_index(self, filepath: str = "rag_index.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ —Ñ–∞–π–ª"""
        try:
            data = {
                "vectors": [(sid, vec.tolist()) for sid, vec in self.startup_vectors],
                "texts": self.startup_texts
            }
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
            logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    def load_index(self, filepath: str = "rag_index.json") -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            self.startup_vectors = [(sid, np.array(vec)) for sid, vec in data["vectors"]]
            self.startup_texts = data["texts"]
            
            logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filepath}: {len(self.startup_vectors)} —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤")
            return True
        except FileNotFoundError:
            logger.warning(f"–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return False

