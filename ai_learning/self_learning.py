"""
–ú–æ–¥—É–ª—å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è AutoScoutBot
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""
import logging
import sqlite3
from typing import List, Dict, Tuple
import json
from collections import defaultdict
import re

logger = logging.getLogger(__name__)

class SelfLearningEngine:
    """
    –î–≤–∏–∂–æ–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
    
    –ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
    1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (AI relevance > 80)
    2. –í—ã—è–≤–ª—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ ‚Üí –∫–∞–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
    3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç few-shot –ø—Ä–∏–º–µ—Ä—ã
    4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    5. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è fine-tuning
    """
    
    def __init__(self, db_path: str = "query_history.db", min_samples: int = None):
        self.db_path = db_path
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.py
        try:
            from config import SELF_LEARNING, FINE_TUNING
            self.min_samples = min_samples or SELF_LEARNING.get('min_samples', 5)
            self.min_ai_relevance = SELF_LEARNING.get('min_ai_relevance', 80)
            self.max_patterns = SELF_LEARNING.get('max_patterns', 50)
            self.max_few_shot = SELF_LEARNING.get('max_few_shot_examples', 10)
            self.fine_tuning_min = FINE_TUNING.get('min_examples', 100)
            self.fine_tuning_output = FINE_TUNING.get('output_file', 'finetuning_dataset.jsonl')
        except ImportError:
            self.min_samples = min_samples or 5
            self.min_ai_relevance = 80
            self.max_patterns = 50
            self.max_few_shot = 10
            self.fine_tuning_min = 100
            self.fine_tuning_output = 'finetuning_dataset.jsonl'
        
    def analyze_and_learn(self) -> Dict:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—É—á–∞–µ—Ç—Å—è
        
        Returns:
            –û—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ
        """
        logger.info("üß† –ó–∞–ø—É—Å–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è...")
        
        report = {
            "patterns_discovered": 0,
            "patterns_updated": 0,
            "synonyms_generated": 0,
            "few_shot_created": 0,
            "recommendations": []
        }
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö
            cursor.execute("SELECT COUNT(*) FROM queries")
            total_queries = cursor.fetchone()[0]
            
            if total_queries < self.min_samples:
                logger.info(f"‚è≥ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {total_queries}/{self.min_samples}")
                report["recommendations"].append(
                    f"–ù–∞–∫–æ–ø–∏—Ç–µ –º–∏–Ω–∏–º—É–º {self.min_samples} –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
                )
                conn.close()
                return report
            
            # 2. –í—ã—è–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            patterns = self._discover_patterns(cursor)
            report["patterns_discovered"] = len(patterns)
            
            # 3. –û–±–Ω–æ–≤–ª—è–µ–º/—Å–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –ë–î
            for pattern in patterns:
                if self._update_or_create_pattern(cursor, pattern):
                    report["patterns_updated"] += 1
            
            # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            synonyms = self._generate_synonyms(cursor)
            report["synonyms_generated"] = len(synonyms)
            
            # 5. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ few-shot –ø—Ä–∏–º–µ—Ä—ã
            few_shot_examples = self._create_few_shot_examples(cursor, patterns)
            report["few_shot_created"] = len(few_shot_examples)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º few-shot –ø—Ä–∏–º–µ—Ä—ã
            if few_shot_examples:
                self._save_few_shot_examples(few_shot_examples)
            
            conn.commit()
            conn.close()
            
            # 6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            report["recommendations"] = self._generate_recommendations(
                total_queries, patterns, synonyms
            )
            
            logger.info(f"‚úÖ –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: "
                       f"{report['patterns_discovered']} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, "
                       f"{report['few_shot_created']} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è: {e}")
            report["error"] = str(e)
        
        return report
    
    def _discover_patterns(self, cursor) -> List[Dict]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        
        # –ù–∞—Ö–æ–¥–∏–º —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (AI relevance > min_ai_relevance)
        cursor.execute("""
            SELECT 
                q.query_text,
                r.cluster,
                r.technologies,
                r.ai_relevance,
                COUNT(*) as frequency
            FROM queries q
            JOIN query_results r ON q.id = r.query_id
            WHERE r.ai_relevance >= ?
            GROUP BY q.query_text, r.cluster
            HAVING COUNT(*) >= ?
            ORDER BY frequency DESC, r.ai_relevance DESC
        """, (self.min_ai_relevance, self.min_samples))
        
        rows = cursor.fetchall()
        
        patterns = []
        for query_text, cluster, technologies, ai_relevance, frequency in rows:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(query_text)
            
            pattern = {
                "query_type": self._categorize_query(query_text),
                "keywords": ", ".join(keywords[:10]),
                "relevant_clusters": cluster,
                "relevant_technologies": technologies or "",
                "example_query": query_text,
                "success_rate": ai_relevance / 100,
                "frequency": frequency
            }
            
            patterns.append(pattern)
        
        logger.info(f"üîç –í—ã—è–≤–ª–µ–Ω–æ {len(patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        return patterns
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '—Ç–∞–∫–∂–µ', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ',
            '–¥–æ', '–ø–æ—Å–ª–µ', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∫–æ–º–ø–∞–Ω–∏—è', '–ø—Ä–æ–µ–∫—Ç', '–æ–±–ª–∞—Å—Ç—å', '—Å—Ñ–µ—Ä–∞'
        }
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        keywords = [w for w in words if w not in stop_words and len(w) > 3]
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã
        from collections import Counter
        word_freq = Counter(keywords)
        
        return [word for word, _ in word_freq.most_common(10)]
    
    def _categorize_query(self, query_text: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query_text.lower()
        
        categories = {
            "ai_ml": ["ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "ml"],
            "clean_tech": ["—ç–∫–æ–ª–æ–≥–∏—è", "–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞", "—É—Å—Ç–æ–π—á–∏–≤–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ", "clean tech", "–∑–µ–ª–µ–Ω—ã–µ"],
            "medtech": ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "—Ç–µ–ª–µ–º–µ–¥–∏—Ü–∏–Ω–∞"],
            "fintech": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "–±–∞–Ω–∫", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "–ø–ª–∞—Ç–µ–∂–∏"],
            "energy": ["—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "—ç–ª–µ–∫—Ç—Ä–æ", "—Å–æ–ª–Ω–µ—á–Ω", "–≤–æ–¥–æ—Ä–æ–¥"],
            "agro": ["—Å–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ", "–∞–≥—Ä–æ", "—Ñ–µ—Ä–º–µ—Ä", "—Ä–∞—Å—Ç–µ–Ω–∏"],
            "robotics": ["—Ä–æ–±–æ—Ç", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–¥—Ä–æ–Ω", "–±–ø–ª–∞", "–±–µ—Å–ø–∏–ª–æ—Ç–Ω"],
        }
        
        for category, keywords in categories.items():
            if any(kw in query_lower for kw in keywords):
                return category
        
        return "general"
    
    def _update_or_create_pattern(self, cursor, pattern: Dict) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –≤ –ë–î"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
            cursor.execute("""
                SELECT id, usage_count, success_rate FROM query_patterns
                WHERE query_type = ? AND keywords = ?
            """, (pattern["query_type"], pattern["keywords"]))
            
            existing = cursor.fetchone()
            
            if existing:
                pattern_id, usage_count, old_success_rate = existing
                
                # –û–±–Ω–æ–≤–ª—è–µ–º (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è success_rate)
                new_success_rate = (old_success_rate + pattern["success_rate"]) / 2
                
                cursor.execute("""
                    UPDATE query_patterns
                    SET relevant_clusters = ?,
                        relevant_technologies = ?,
                        example_query = ?,
                        success_rate = ?,
                        usage_count = usage_count + ?
                    WHERE id = ?
                """, (
                    pattern["relevant_clusters"],
                    pattern["relevant_technologies"],
                    pattern["example_query"],
                    new_success_rate,
                    pattern["frequency"],
                    pattern_id
                ))
                
                logger.info(f"üìù –û–±–Ω–æ–≤–ª–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern['query_type']}")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                cursor.execute("""
                    INSERT INTO query_patterns
                    (query_type, keywords, relevant_clusters, relevant_technologies,
                     example_query, example_startups, success_rate, usage_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    pattern["query_type"],
                    pattern["keywords"],
                    pattern["relevant_clusters"],
                    pattern["relevant_technologies"],
                    pattern["example_query"],
                    "",  # example_startups –∑–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
                    pattern["success_rate"],
                    pattern["frequency"]
                ))
                
                logger.info(f"‚ú® –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern['query_type']}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {e}")
            return False
    
    def _generate_synonyms(self, cursor) -> Dict[str, List[str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        cursor.execute("""
            SELECT DISTINCT
                q1.query_text,
                q2.query_text,
                r1.cluster,
                AVG(r1.ai_relevance) as avg_relevance
            FROM queries q1
            JOIN query_results r1 ON q1.id = r1.query_id
            JOIN query_results r2 ON r1.startup_name = r2.startup_name
            JOIN queries q2 ON r2.query_id = q2.id
            WHERE q1.id != q2.id
              AND r1.ai_relevance >= 70
              AND r2.ai_relevance >= 70
            GROUP BY q1.query_text, q2.query_text, r1.cluster
            HAVING COUNT(*) >= 2
        """)
        
        rows = cursor.fetchall()
        
        synonyms = defaultdict(set)
        for query1, query2, cluster, avg_rel in rows:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords1 = set(self._extract_keywords(query1))
            keywords2 = set(self._extract_keywords(query2))
            
            # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            unique1 = keywords1 - keywords2
            unique2 = keywords2 - keywords1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ —Å–∏–Ω–æ–Ω–∏–º—ã
            for word1 in unique1:
                for word2 in unique2:
                    synonyms[word1].add(word2)
                    synonyms[word2].add(word1)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ–±—ã—á–Ω—ã–π dict
        synonyms_dict = {k: list(v) for k, v in synonyms.items() if len(v) > 0}
        
        logger.info(f"üî§ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–∏–Ω–æ–Ω–∏–º–æ–≤: {len(synonyms_dict)}")
        return synonyms_dict
    
    def _create_few_shot_examples(self, cursor, patterns: List[Dict]) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        
        examples = []
        
        for pattern in patterns[:10]:  # –¢–æ–ø-10 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            cursor.execute("""
                SELECT r.startup_name, r.ai_relevance, r.cluster, r.technologies
                FROM queries q
                JOIN query_results r ON q.id = r.query_id
                WHERE q.query_text = ?
                ORDER BY r.ai_relevance DESC
                LIMIT 5
            """, (pattern["example_query"],))
            
            relevant = cursor.fetchall()
            
            if len(relevant) < 2:
                continue
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä
            example = {
                "category": pattern["query_type"],
                "query": pattern["example_query"],
                "relevant": [
                    f"{name} ({cluster})"
                    for name, rel, cluster, _ in relevant if rel >= 80
                ],
                "clusters": list(set([r[2] for r in relevant if r[1] >= 80])),
                "keywords": pattern["keywords"].split(", ")
            }
            
            examples.append(example)
        
        logger.info(f"üìö –°–æ–∑–¥–∞–Ω–æ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤: {len(examples)}")
        return examples
    
    def _save_few_shot_examples(self, examples: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —Ñ–∞–π–ª"""
        try:
            output_file = "ai_learning/learned_examples.py"
            
            content = '''"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ few-shot –ø—Ä–∏–º–µ—Ä—ã
–°–æ–∑–¥–∞–Ω–æ —Å–∏—Å—Ç–µ–º–æ–π —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
"""

LEARNED_EXAMPLES = {
'''
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            by_category = defaultdict(list)
            for ex in examples:
                by_category[ex["category"]].append(ex)
            
            for category, category_examples in by_category.items():
                content += f'    "{category}": {{\n'
                content += f'        "description": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è",\n'
                content += f'        "examples": [\n'
                
                for ex in category_examples:
                    content += '            {\n'
                    content += f'                "query": "{ex["query"]}",\n'
                    content += f'                "relevant": {json.dumps(ex["relevant"], ensure_ascii=False)},\n'
                    content += f'                "clusters": {json.dumps(ex["clusters"], ensure_ascii=False)},\n'
                    content += f'                "keywords": {json.dumps(ex["keywords"], ensure_ascii=False)}\n'
                    content += '            },\n'
                
                content += '        ]\n'
                content += '    },\n'
            
            content += '}\n'
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"üíæ Few-shot –ø—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è few-shot: {e}")
    
    def _generate_recommendations(self, total_queries: int, 
                                  patterns: List[Dict], 
                                  synonyms: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if total_queries < 50:
            recommendations.append(
                f"üìä –ù–∞–∫–æ–ø–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö: {total_queries}/50 –∑–∞–ø—Ä–æ—Å–æ–≤. "
                f"–î–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 50."
            )
        elif total_queries < 200:
            recommendations.append(
                f"üìä –•–æ—Ä–æ—à–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å: {total_queries}/200 –∑–∞–ø—Ä–æ—Å–æ–≤. "
                f"–ü—Ä–∏ 200+ —Å–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
            )
        else:
            recommendations.append(
                f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤. "
                f"–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∞ —Ö–æ—Ä–æ—à–æ!"
            )
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if len(patterns) < 5:
            recommendations.append(
                f"üîç –í—ã—è–≤–ª–µ–Ω–æ –º–∞–ª–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ ({len(patterns)}). "
                f"–î–µ–ª–∞–π—Ç–µ –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã."
            )
        else:
            recommendations.append(
                f"‚úÖ –í—ã—è–≤–ª–µ–Ω–æ {len(patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –°–∏—Å—Ç–µ–º–∞ —É—á–∏—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ!"
            )
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        if len(synonyms) < 10:
            recommendations.append(
                f"üî§ –ú–∞–ª–æ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ ({len(synonyms)}). "
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤."
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ fine-tuning
        if total_queries >= 100:
            recommendations.append(
                "üöÄ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fine-tuning! "
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python ai_learning/train_model.py"
            )
        
        return recommendations
    
    def export_for_finetuning(self, output_file: str = None) -> int:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fine-tuning GigaChat
        
        –§–æ—Ä–º–∞—Ç JSONL (JSON Lines):
        {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
        """
        try:
            if output_file is None:
                output_file = self.fine_tuning_output
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (AI relevance >= min_ai_relevance - 10)
            cursor.execute("""
                SELECT 
                    q.query_text,
                    r.startup_name,
                    r.cluster,
                    r.technologies,
                    r.ai_relevance,
                    r.rag_similarity
                FROM queries q
                JOIN query_results r ON q.id = r.query_id
                WHERE r.ai_relevance >= ?
                ORDER BY r.ai_relevance DESC
            """, (max(70, self.min_ai_relevance - 10),))
            
            rows = cursor.fetchall()
            conn.close()
            
            if len(rows) < self.fine_tuning_min:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fine-tuning: {len(rows)}/{self.fine_tuning_min}")
                return 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            with open(output_file, 'w', encoding='utf-8') as f:
                for query, startup, cluster, tech, ai_rel, rag_sim in rows:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è fine-tuning
                    example = {
                        "messages": [
                            {
                                "role": "user",
                                "content": f"""–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å—Ç–∞—Ä—Ç–∞–ø–∞ –∑–∞–ø—Ä–æ—Å—É –æ—Ç 0 –¥–æ 100.

–ó–∞–ø—Ä–æ—Å: {query}

–°—Ç–∞—Ä—Ç–∞–ø:
–ù–∞–∑–≤–∞–Ω–∏–µ: {startup}
–ö–ª–∞—Å—Ç–µ—Ä: {cluster}
–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {tech or '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 100):"""
                            },
                            {
                                "role": "assistant",
                                "content": str(int(ai_rel))
                            }
                        ]
                    }
                    
                    f.write(json.dumps(example, ensure_ascii=False) + '\n')
            
            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(rows)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è fine-tuning: {output_file}")
            return len(rows)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–ª—è fine-tuning: {e}")
            return 0

