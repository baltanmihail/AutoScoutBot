"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
–õ–µ–≥–∫–æ–µ –∏ –±—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""
import logging
import sqlite3
from typing import Dict, List
from collections import defaultdict

logger = logging.getLogger(__name__)

class IncrementalLearner:
    """
    –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    
    –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (Self Learning Engine):
    - –†–∞–±–æ—Ç–∞–µ—Ç –ë–´–°–¢–†–û (< 1 —Å–µ–∫—É–Ω–¥—ã)
    - –û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å
    - –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã
    - –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—á–µ—Ç—á–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ë–î
    
    –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ N –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è:
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤
    - –°–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤
    - –≠–∫—Å–ø–æ—Ä—Ç–∞ –¥–ª—è fine-tuning
    """
    
    def __init__(self, db_path: str = "query_history.db"):
        self.db_path = db_path
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        try:
            from config import SELF_LEARNING
            self.min_ai_relevance = SELF_LEARNING.get('min_ai_relevance', 80)
        except ImportError:
            self.min_ai_relevance = 80
    
    def learn_from_query(self, query_id: int) -> Dict:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            query_id: ID –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ query_history.db
            
        Returns:
            –û—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ
        """
        report = {
            "patterns_updated": 0,
            "insights_gained": [],
            "quality_assessment": "",
        }
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–µ
            cursor.execute("""
                SELECT query_text, model_type, expanded_query
                FROM queries
                WHERE id = ?
            """, (query_id,))
            
            query_row = cursor.fetchone()
            if not query_row:
                return report
            
            query_text, model_type, expanded_query = query_row
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            cursor.execute("""
                SELECT startup_name, cluster, technologies, 
                       rag_similarity, ai_relevance
                FROM query_results
                WHERE query_id = ?
                ORDER BY ai_relevance DESC
            """, (query_id,))
            
            results = cursor.fetchall()
            
            if not results:
                return report
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            avg_relevance = sum(r[4] or 0 for r in results) / len(results)
            best_relevance = max(r[4] or 0 for r in results)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
            if best_relevance >= self.min_ai_relevance:
                # –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å - –æ–±–Ω–æ–≤–ª—è–µ–º/—Å–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
                self._update_pattern_incremental(
                    cursor, query_text, results, avg_relevance
                )
                report["patterns_updated"] = 1
                report["quality_assessment"] = "‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
                report["insights_gained"].append(
                    f"–ó–∞–ø—Ä–æ—Å '{query_text[:50]}...' –¥–∞–ª —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (AI={best_relevance:.0f})"
                )
            elif avg_relevance >= 60:
                report["quality_assessment"] = "üü° –°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
                report["insights_gained"].append(
                    f"–ó–∞–ø—Ä–æ—Å –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å: —Å—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {avg_relevance:.0f}/100"
                )
            else:
                report["quality_assessment"] = "üî¥ –ü–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
                report["insights_gained"].append(
                    f"–ó–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏: –Ω–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {avg_relevance:.0f}/100"
                )
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
            successful_clusters = [r[1] for r in results if r[4] and r[4] >= self.min_ai_relevance]
            if successful_clusters:
                most_common = max(set(successful_clusters), key=successful_clusters.count)
                report["insights_gained"].append(
                    f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä: {most_common}"
                )
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
            successful_techs = []
            for r in results:
                if r[4] and r[4] >= self.min_ai_relevance and r[2]:
                    successful_techs.append(r[2])
            
            if successful_techs:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
                tech_words = []
                for tech_str in successful_techs[:3]:
                    if tech_str:
                        words = tech_str.split(';')[:2]  # –ü–µ—Ä–≤—ã–µ 2
                        tech_words.extend(words)
                
                if tech_words:
                    report["insights_gained"].append(
                        f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(tech_words[:3])}"
                    )
            
            conn.commit()
            conn.close()
            
            logger.info(f"üìö –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: query_id={query_id}, –∫–∞—á–µ—Å—Ç–≤–æ={report['quality_assessment']}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            report["error"] = str(e)
        
        return report
    
    def _update_pattern_incremental(self, cursor, query_text: str, 
                                    results: List, avg_relevance: float):
        """
        –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            keywords = self._extract_keywords_fast(query_text)
            keywords_str = ", ".join(keywords[:5])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            category = self._categorize_query_fast(query_text)
            
            # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä
            best_cluster = ""
            for r in results:
                if r[4] and r[4] >= self.min_ai_relevance and r[1]:
                    best_cluster = r[1]
                    break
            
            if not best_cluster:
                return
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            cursor.execute("""
                SELECT id, usage_count, success_rate 
                FROM query_patterns
                WHERE query_type = ? AND keywords = ?
            """, (category, keywords_str))
            
            existing = cursor.fetchone()
            
            if existing:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ)
                pattern_id, usage_count, old_success_rate = existing
                
                # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                new_success_rate = (old_success_rate * 0.9 + (avg_relevance / 100) * 0.1)
                
                cursor.execute("""
                    UPDATE query_patterns
                    SET usage_count = usage_count + 1,
                        success_rate = ?,
                        relevant_clusters = ?
                    WHERE id = ?
                """, (new_success_rate, best_cluster, pattern_id))
                
                logger.debug(f"–ü–∞—Ç—Ç–µ—Ä–Ω –æ–±–Ω–æ–≤–ª–µ–Ω: {category} (usage={usage_count+1})")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                cursor.execute("""
                    INSERT INTO query_patterns
                    (query_type, keywords, relevant_clusters, relevant_technologies,
                     example_query, example_startups, success_rate, usage_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, 1)
                """, (
                    category,
                    keywords_str,
                    best_cluster,
                    "",  # –ó–∞–ø–æ–ª–Ω–∏–º –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
                    query_text[:200],
                    "",  # –ó–∞–ø–æ–ª–Ω–∏–º –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
                    avg_relevance / 100
                ))
                
                logger.info(f"‚ú® –ù–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å–æ–∑–¥–∞–Ω: {category}")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {e}")
    
    def _extract_keywords_fast(self, text: str) -> List[str]:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        import re
        
        stop_words = {'–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '–¥–æ', '–ø–æ'}
        words = re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', text.lower())
        keywords = [w for w in words if w not in stop_words]
        
        return keywords[:5]
    
    def _categorize_query_fast(self, query_text: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è"""
        query_lower = query_text.lower()
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if any(w in query_lower for w in ['ai', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–µ–π—Ä–æ', '–º–∞—à–∏–Ω–Ω–æ–µ']):
            return 'ai_ml'
        elif any(w in query_lower for w in ['—ç–∫–æ–ª–æ–≥–∏—è', '–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞', '—É—Å—Ç–æ–π—á–∏–≤', 'clean']):
            return 'clean_tech'
        elif any(w in query_lower for w in ['–º–µ–¥–∏—Ü–∏–Ω', '–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω', '–¥–∏–∞–≥–Ω–æ—Å—Ç']):
            return 'medtech'
        elif any(w in query_lower for w in ['—Ñ–∏–Ω–∞–Ω—Å', '–±–∞–Ω–∫', '–±–ª–æ–∫—á–µ–π–Ω', '–∫—Ä–∏–ø—Ç–æ']):
            return 'fintech'
        elif any(w in query_lower for w in ['—ç–Ω–µ—Ä–≥–µ—Ç', '—ç–ª–µ–∫—Ç—Ä–æ', '—Å–æ–ª–Ω–µ—á–Ω']):
            return 'energy'
        elif any(w in query_lower for w in ['—Ä–æ–±–æ—Ç', '–¥—Ä–æ–Ω', '–±–ø–ª–∞', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑']):
            return 'robotics'
        else:
            return 'general'
    
    def get_quick_stats(self) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor.execute("""
                SELECT q.id, q.query_text, AVG(r.ai_relevance) as avg_rel
                FROM queries q
                JOIN query_results r ON q.id = r.query_id
                WHERE r.ai_relevance > 0
                GROUP BY q.id
                ORDER BY q.timestamp DESC
                LIMIT 5
            """)
            
            recent_queries = cursor.fetchall()
            conn.close()
            
            return {
                "recent_queries": [
                    {
                        "id": q[0],
                        "text": q[1][:50] + "...",
                        "avg_relevance": q[2]
                    }
                    for q in recent_queries
                ]
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

